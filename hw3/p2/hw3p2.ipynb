{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README"
      ],
      "metadata": {
        "id": "UQf7a49nHVD6"
      },
      "id": "UQf7a49nHVD6"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "To run the entire pipeline : Runtime -> Rull All OR Ctrl + F9\n",
        "\n",
        "Experimentation :\n",
        "\n",
        "    1) Epochs            : 10       : the model was able to converge within very small number of epochs\n",
        "    2) Batch Size        : 128      : maximum that fits the hardware\n",
        "    3) Beam Width        : 3        : increasing beam width also increased the training time\n",
        "    4) Weight Decay      : 0.05     : value was set to commnly used value\n",
        "    5) Scheduler         : Constant : constant learning rate gave satisfactory results\n",
        "    6) Optimizer         : AdamW    : sgd, adam, and adamw were tried on small networks and adamw gave slightly better results\n",
        "    \n",
        "    7) Network           : 1D convolution layers were added to first extract features and then pass to the lstm. this improved the accuracy by a large value\n",
        "                           [1, 2, 3, 4] layers of lstm were tried and 4 layer lstm gave better results. bidirectional lstm was used\n",
        "                           finally the lstm outputs were passed through 2 linear layers\n",
        "\n",
        "                           Following is the network used :\n",
        "                           \n",
        "                           Network(\n",
        "                            (embedding): Sequential(\n",
        "                              (0): Conv1d(15, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
        "                              (1): GELU(approximate=none)\n",
        "                              (2): Dropout(p=0.3, inplace=False)\n",
        "                              (3): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
        "                              (4): GELU(approximate=none)\n",
        "                            )\n",
        "                            (lstm): LSTM(256, 512, num_layers=4, batch_first=True, bidirectional=True)\n",
        "                            (classification): Sequential(\n",
        "                              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "                              (1): GELU(approximate=none)\n",
        "                              (2): Linear(in_features=1024, out_features=43, bias=True)\n",
        "                            )\n",
        "                            (logSoftmax): LogSoftmax(dim=2)\n",
        "                          )\n",
        "\n",
        "    9) WandB Runs        : wandb project is made public and can be found here : https://wandb.ai/ajinkyanande111/hw3p2\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "6SaLxloxHXzZ"
      },
      "id": "6SaLxloxHXzZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "iP4PdF3YGI5J"
      },
      "id": "iP4PdF3YGI5J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d56a9cc",
      "metadata": {
        "id": "8d56a9cc"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6fd612",
      "metadata": {
        "id": "0f6fd612"
      },
      "outputs": [],
      "source": [
        "wandb.login(key='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d73caeb",
      "metadata": {
        "id": "7d73caeb"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummaryX -q\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git -q\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install .  -q\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c72e7a5a",
      "metadata": {
        "id": "c72e7a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0792948f-0887-4ade-b4d4-839d95a2fcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import math\n",
        "import gc\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "\n",
        "import ctcdecode\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "import Levenshtein\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31153b5e",
      "metadata": {
        "id": "31153b5e"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w+') as f:\n",
        "    f.write('{\"username\":\"ajinkyanande\",\"key\":\"5f60b9bc169fe67552e51c70e754066d\"}') \n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab0b6dd",
      "metadata": {
        "id": "dab0b6dd"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c 11-785-f22-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c10b127",
      "metadata": {
        "id": "0c10b127"
      },
      "outputs": [],
      "source": [
        "!unzip -q 11-785-f22-hw3p2.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Gc9_7OCmGOPN"
      },
      "id": "Gc9_7OCmGOPN"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7fb22ff2",
      "metadata": {
        "id": "7fb22ff2"
      },
      "outputs": [],
      "source": [
        "config = {'beam_width': 3,\n",
        "          'lr': 0.001,\n",
        "          'epochs': 10,\n",
        "          'batch_size': 128}\n",
        "\n",
        "root = '/content/hw3p2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "97dfcb85",
      "metadata": {
        "id": "97dfcb85"
      },
      "outputs": [],
      "source": [
        "CMUdict_ARPAbet = {\"\" : \" \",\n",
        "                   \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "                   \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "                   \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "                   \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "                   \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "                   \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "                   \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "                   \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "                   \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "phonems = CMUdict\n",
        "mapping = CMUdict_ARPAbet\n",
        "labels = ARPAbet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "_6rChnA3GSsW"
      },
      "id": "_6rChnA3GSsW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b6f8ce",
      "metadata": {
        "id": "58b6f8ce"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, mfcc_dir, transcript_dir): \n",
        "\n",
        "        # mfcc and transcript file names\n",
        "        mfcc_files = os.listdir(mfcc_dir)\n",
        "        transcript_files = os.listdir(transcript_dir)\n",
        "\n",
        "        # dataset length\n",
        "        assert len(mfcc_files) == len(transcript_files)\n",
        "        self.length = len(mfcc_files)\n",
        "        \n",
        "        # iterate through files and load X and Y in list\n",
        "        self.mfccs = []\n",
        "        self.transcripts = []\n",
        "\n",
        "        for i in tqdm(range(self.length), total=self.length):\n",
        "            \n",
        "            # load a single mfcc\n",
        "            mfcc = np.load(mfcc_dir + mfcc_files[i])\n",
        "\n",
        "            # cepstral normalization of mfcc\n",
        "            mfcc = mfcc - np.mean(mfcc, axis=0, keepdims=True)\n",
        "            mfcc = mfcc / np.std(mfcc, axis=0, keepdims=True)\n",
        "\n",
        "            # load the corresponding transcript\n",
        "            transcript = np.load(transcript_dir + transcript_files[i])\n",
        "\n",
        "            # remove start of line [SOS] and end of line [EOS]\n",
        "            transcript = transcript[1: -1]\n",
        "\n",
        "            # phonems to dictionary indexes\n",
        "            transcript_1 = [phonems.index(y) for y in transcript]\n",
        "            \n",
        "            # append each mfcc to self.mfcc and transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript_1)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        return torch.FloatTensor(self.mfccs[ind]), torch.FloatTensor(self.transcripts[ind])\n",
        "\n",
        "    def collate_fn(batch):\n",
        "\n",
        "        # batch of input mfcc coefficients and transcripts\n",
        "        batch_mfcc = [x[0] for x in batch]\n",
        "        batch_transcript = [y[1] for y in batch]\n",
        "\n",
        "        # pad mfccs and transcripts of batch to make of same length\n",
        "        lengths_mfcc = [len(x) for x in batch_mfcc]\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "\n",
        "        lengths_transcript = [len(y) for y in batch_transcript]\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "        \n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e82981",
      "metadata": {
        "id": "f7e82981"
      },
      "outputs": [],
      "source": [
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "   \n",
        "    def __init__(self, mfcc_dir): \n",
        "\n",
        "        # mfcc file names\n",
        "        mfcc_files = sorted(os.listdir(mfcc_dir))\n",
        "\n",
        "        # dataset length\n",
        "        self.length = len(mfcc_files)\n",
        "        \n",
        "        # iterate through files and load X in list\n",
        "        self.mfccs = []\n",
        "\n",
        "        for i in tqdm(range(self.length), total=self.length):\n",
        "            \n",
        "            # load a single mfcc\n",
        "            mfcc = np.load(mfcc_dir + mfcc_files[i])\n",
        "\n",
        "            # cepstral normalization of mfcc\n",
        "            mfcc = mfcc - np.mean(mfcc, axis=0, keepdims=True)\n",
        "            mfcc = mfcc / np.std(mfcc, axis=0, keepdims=True)\n",
        "\n",
        "            # append each mfcc to self.mfcc\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        return torch.FloatTensor(self.mfccs[ind])\n",
        "\n",
        "    def collate_fn(batch):\n",
        "\n",
        "        # batch of input mfcc coefficients and transcripts\n",
        "        batch_mfcc = [x for x in batch]\n",
        "\n",
        "        # pad mfccs and transcripts of batch to make of same length\n",
        "        lengths_mfcc = [len(x) for x in batch_mfcc]\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "        \n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "QiOa9FbrGX6u"
      },
      "id": "QiOa9FbrGX6u"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d2d463af",
      "metadata": {
        "id": "d2d463af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e670487d-170d-4fd1-be06-620a37a6f667"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a395f20f",
      "metadata": {
        "id": "a395f20f"
      },
      "outputs": [],
      "source": [
        "train_mfcc_dir       = '/content/hw3p2/train-clean-360/mfcc/'\n",
        "train_transcript_dir = '/content/hw3p2/train-clean-360/transcript/raw/'\n",
        "\n",
        "val_mfcc_dir         = '/content/hw3p2/dev-clean/mfcc/'\n",
        "val_transcript_dir   = '/content/hw3p2/dev-clean/transcript/raw/'\n",
        "\n",
        "test_mfcc_dir        = '/content/hw3p2/test-clean/mfcc/'\n",
        "\n",
        "train_data = AudioDataset(train_mfcc_dir, train_transcript_dir)\n",
        "val_data   = AudioDataset(val_mfcc_dir, val_transcript_dir)\n",
        "test_data  = AudioDatasetTest(test_mfcc_dir)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=config['batch_size'],\n",
        "                                           collate_fn=AudioDataset.collate_fn,\n",
        "                                           pin_memory=True,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader   = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=config['batch_size'],\n",
        "                                           collate_fn=AudioDataset.collate_fn,\n",
        "                                           pin_memory=True,\n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader  = torch.utils.data.DataLoader(test_data,\n",
        "                                           batch_size=config['batch_size'],\n",
        "                                           collate_fn=AudioDatasetTest.collate_fn,\n",
        "                                           pin_memory=True,\n",
        "                                           shuffle=False)\n",
        "\n",
        "print(\"\\nBatch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5580f2f8",
      "metadata": {
        "id": "5580f2f8"
      },
      "outputs": [],
      "source": [
        "# SANITY CHECK\n",
        "\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break\n",
        "\n",
        "for data in test_loader:\n",
        "    x, lx = data\n",
        "    print(x.shape, lx.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a4a4ddf4",
      "metadata": {
        "id": "a4a4ddf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb00729-4050-4645-be2c-3d4c438bcb46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "OUT_SIZE = len(labels)\n",
        "OUT_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "WVuko-B6GfrA"
      },
      "id": "WVuko-B6GfrA"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b580077e",
      "metadata": {
        "id": "b580077e"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Sequential(nn.Conv1d(in_channels=15, out_channels=128, kernel_size=3, padding=1, stride=2),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Dropout(p=0.3),\n",
        "                                       nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2),\n",
        "                                       nn.GELU())\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=4, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.classification = nn.Sequential(nn.Linear(1024 , 1024),\n",
        "                                            nn.GELU(),\n",
        "                                            nn.Linear(1024, OUT_SIZE))\n",
        "        \n",
        "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "\n",
        "        x_embd = self.embedding(x.transpose(1, 2)).transpose(1, 2)\n",
        "        lx = (((lx - 1) // 2) - 1) // 2\n",
        "    \n",
        "        x_packed = pack_padded_sequence(x_embd, lx, batch_first=True, enforce_sorted=False)\n",
        "        out_packed, _ = self.lstm(x_packed)\n",
        "\n",
        "        out_unpacked, lens_unpacked = pad_packed_sequence(out_packed, batch_first=True)\n",
        "        out = self.classification(out_unpacked)\n",
        "        \n",
        "        out = self.logSoftmax(out)\n",
        "\n",
        "        return out, lens_unpacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6da4a4f0",
      "metadata": {
        "id": "6da4a4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578d1594-97dc-4335-80c5-ba6c2e3f0d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (embedding): Sequential(\n",
            "    (0): Conv1d(15, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): GELU(approximate=none)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (4): GELU(approximate=none)\n",
            "  )\n",
            "  (lstm): LSTM(256, 512, num_layers=4, batch_first=True, bidirectional=True)\n",
            "  (classification): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): GELU(approximate=none)\n",
            "    (2): Linear(in_features=1024, out_features=43, bias=True)\n",
            "  )\n",
            "  (logSoftmax): LogSoftmax(dim=2)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "GkRtmql0GleQ"
      },
      "id": "GkRtmql0GleQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d36908",
      "metadata": {
        "id": "c5d36908"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CTCLoss(blank=0)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=0.0)\n",
        "\n",
        "decoder = CTCBeamDecoder(labels=labels,\n",
        "                         beam_width=config['beam_width'],\n",
        "                         blank_id=0,\n",
        "                         log_probs_input=True)\n",
        "\n",
        "# mixed Precision\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba12b738",
      "metadata": {
        "id": "ba12b738"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(name='submission',\n",
        "                 reinit=True,\n",
        "                 project='hw3p2',\n",
        "                 config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10882d68",
      "metadata": {
        "id": "10882d68"
      },
      "outputs": [],
      "source": [
        "def calculate_levenshtein(h, y, lh, ly, decoder, labels):\n",
        "\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(h, seq_lens=lh)\n",
        "\n",
        "    assert lh.shape == ly.shape\n",
        "    batch_size = lh.shape[0]\n",
        "\n",
        "    distance = 0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        prediction = beam_results[i, 0, :out_lens[i, 0]]\n",
        "        \n",
        "        prediction = ''.join([labels[n] for n in prediction])\n",
        "        target = ''.join([labels[n] for n in y.int()[i, :ly[i]]])\n",
        "        \n",
        "        distance += Levenshtein.distance(target, prediction)\n",
        "        \n",
        "    distance /= batch_size\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab8dda4",
      "metadata": {
        "id": "cab8dda4"
      },
      "outputs": [],
      "source": [
        "# SANITY CHECK\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      \n",
        "        x, y, lx, ly = data\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        out, lout = model(x, lx)\n",
        "        \n",
        "        print(x.shape)\n",
        "        print(lx.shape)\n",
        "        print(y.shape)\n",
        "        print(ly.shape)\n",
        "        print(out.shape)\n",
        "        print(lout.shape)\n",
        "\n",
        "        loss = criterion(out.transpose(1, 0), y, lout, ly)\n",
        "        print(f'loss: ', loss)\n",
        "\n",
        "        distance = calculate_levenshtein(out, y, lout, ly, decoder, labels)\n",
        "        print(f'lev-distance: ', distance)\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "4IDpxPRjG7CF"
      },
      "id": "4IDpxPRjG7CF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7555d802",
      "metadata": {
        "id": "7555d802"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer, criterion, scheduler, scaler):\n",
        "\n",
        "    # train mode\n",
        "    model.train()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='train') \n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # forward prop\n",
        "        out, lout = model(x, lx)\n",
        "\n",
        "        # loss calc\n",
        "        loss = criterion(out.transpose(1, 0), y, lout, ly)\n",
        "        train_loss += loss\n",
        "\n",
        "        # init gradient for each batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # back prop with mixed precision\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # scaler update\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.set_postfix(loss=f\"{train_loss/ (i+1):.4f}\",\n",
        "                              lr=f\"{optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56e8556",
      "metadata": {
        "id": "b56e8556"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model):\n",
        "    \n",
        "    # eval mode\n",
        "    model.eval()\n",
        "\n",
        "    val_dist = 0\n",
        "    val_loss = 0\n",
        "    \n",
        "    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, leave=True, position=0, desc='val') \n",
        "\n",
        "    for i, data in enumerate(data_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # forward prop\n",
        "        with torch.inference_mode():\n",
        "            out, lout = model(x, lx)\n",
        "\n",
        "        # loss calc\n",
        "        loss = criterion(out.transpose(1, 0), y, lout, ly)\n",
        "        val_loss += loss\n",
        "\n",
        "        # levenshtein distance\n",
        "        dist = calculate_levenshtein(out, y, lout, ly, decoder, labels)\n",
        "        val_dist += dist\n",
        "\n",
        "        batch_bar.set_postfix(loss=f\"{val_loss/ (i+1):.4f}\",\n",
        "                              lr=f\"{optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    val_loss /= len(data_loader)\n",
        "    val_dist /= len(data_loader)\n",
        "\n",
        "    return val_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4bda3c",
      "metadata": {
        "id": "ab4bda3c"
      },
      "outputs": [],
      "source": [
        "best_val_dist = math.inf\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    # clear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    train_loss = train(train_loader, model, optimizer, criterion, None, scaler)\n",
        "    val_loss, val_dist = evaluate(val_loader, model)\n",
        "\n",
        "    # print updates\n",
        "    print('epoch {}/{}'.format(epoch+1, config['epochs']))\n",
        "    print('learning Rate: {:.6f}'.format(optimizer.param_groups[0]['lr']))\n",
        "    print('train loss: {:.4f}'.format(train_loss))\n",
        "    print('val loss: {:.4f}'.format(val_loss))\n",
        "    print('val dist: {:.2f}%'.format(val_dist))\n",
        "    \n",
        "    if val_dist < best_val_dist:\n",
        "\n",
        "        print('saving model')\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'val_dist': val_dist,\n",
        "                    'epoch': epoch}, 'checkpoint.pth')\n",
        "        \n",
        "        best_val_dist = val_dist\n",
        "        wandb.save('checkpoint.pth')\n",
        "    \n",
        "    wandb.log({'train_loss': train_loss,\n",
        "               'val_loss': val_loss,\n",
        "               'val_dist': val_dist})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('loading best model')\n",
        "checkpoint = torch.load('checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "decoder_test = CTCBeamDecoder(labels=labels,\n",
        "                              beam_width=20,\n",
        "                              blank_id=0,\n",
        "                              log_probs_input=True)\n",
        "\n",
        "def make_output(h, lh, decoder, labels):\n",
        "  \n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(h, seq_lens=lh)\n",
        "    batch_size = lh.shape[0]\n",
        "\n",
        "    dist = 0\n",
        "    preds = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        h_sliced = beam_results[i, 0, :out_seq_len[i, 0]]\n",
        "        h_string = ''.join([labels[n] for n in h_sliced])\n",
        "\n",
        "        preds.append(h_string)\n",
        "    \n",
        "    return preds"
      ],
      "metadata": {
        "id": "lp8M6fCgvU1W"
      },
      "id": "lp8M6fCgvU1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(data_loader, model, decoder, labels):\n",
        "\n",
        "    # eval mode\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, leave=False, position=0, desc='Val') \n",
        "\n",
        "    preds = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        for i, data in enumerate(data_loader):\n",
        "\n",
        "            x, lx = data\n",
        "            x = x.to(device)\n",
        "\n",
        "            # forward prop\n",
        "            out, lout = model(x, lx)\n",
        "            batch_preds = make_output(out, lout, decoder, labels)\n",
        "\n",
        "            preds += batch_preds\n",
        "\n",
        "    return preds\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "predictions = predict(test_loader, model, decoder, labels)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/hw3p2/test-clean/transcript/random_submission.csv')\n",
        "df.label = predictions\n",
        "\n",
        "df.to_csv('submission.csv', index = False)\n",
        "!kaggle competitions submit -c 11-785-f22-hw3p2 -f submission.csv -m \"I made it!\""
      ],
      "metadata": {
        "id": "8WnrxcvlvTpP"
      },
      "id": "8WnrxcvlvTpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rkVj6Cg_2zR"
      },
      "id": "3rkVj6Cg_2zR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}